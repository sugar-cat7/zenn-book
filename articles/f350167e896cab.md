---
title: "新規サービスの立ち上げから10ヶ月経ったので色々まとめてみた"
emoji: "🐙"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: []
published: false
publication_name: aishift
---

こんにちは、[sugar-cat](https://twitter.com/sugar235711)です。

AIShiftでは去年の11月から`AI Worker`^[https://www.ai-shift.co.jp/3958]という新しいサービスの開発が始まりました。（以下AI Worker）
開発が始まり、約10ヶ月で色々あったので私が行った機能開発以外の外部公開が可能な取り組みを雑にまとめました。まだまだ開発途上というところもあり至らない箇所が散見されますが、何かしらの参考や改善点として参考になれば幸いです。

3ヶ月時点での記事はこちらです。ざっくり何を作っていて、どんな技術を使っているかが書いてあります。
https://zenn.dev/aishift/articles/ce9783a0d7acd0


## Google Cloudへのサービス展開
弊チームでは、元々Microsoft Azureのスタック上でアプリケーションを構成しており、運用を行っていました。しかし、ビジネス要件の変更により、Google Cloudと併用してサービスを展開することになりました。
### 元々のAzure構成
元々Backendは下記のようなAzure Container Appsベースで構成されており、インフラ構築の手間を最小限にアプリケーションの開発に注力できるようにしていました。

![alt text](/images/aiworkerv2/1.png)


しかし、開発の途中で`IaC化に伴う問題`や、`Azure固有の知識`に悩む場面が多く、Kubernetes(以下k8s)に移行することを決定しました。

https://zenn.dev/aishift/articles/01ac0622cff568


そこで、元々AI ShiftではGoogle Kubernetes Engine(以下GKE)を利用していたため、まずはGoogle Cloud上での展開と合わせてk8s化を行っていくことにしました。

Google Cloudへの展開にあたり、Azureでの動作環境を再現するため、以下の手順で作業を進めました。

1. **定義済みのAzure Resourceから対応するGoogle Cloud及び、k8sリソースの調査**
2. **Terraform/マニュフェストを利用してリソースの作成**
3. **アプリケーションレイヤーの修正**
4. **CI/CDの修正**

#### 1. 定義済みのAzure Resourceから対応するGoogle Cloud及び、k8sリソースの調査

k8sを使用するにあたり、各リソースはTerraformで管理するのか、マニフェストで管理するのかの棲み分けが必要です。

弊チームでは他のCloud Providerにも移行しやすいような構成が望ましいと考え、以下の方針を採用しました。

- **k8s側に用意されているリソース(ex: Deployment, Service, Ingress等)はマニフェストで管理**
- **ベースとなるGoogle Cloudリソース(ex: Database, Storage, k8sのNode Pool等)はTerraformで管理**を行うことにしました。

また、元々AI Shiftで運用していたClusterに同居する形でリソースを定義し、基本は**k8sレイヤーはアプリケーションのデプロイのライフサイクルに合わせて管理**を行なっています。


#### 2. Terraform/マニュフェストを利用してリソースの作成

まず、弊チームでは1つのリポジトリ上でTerraformとk8sのマニュフェストを管理しています。
現状1Repositoy=1アプリケーションの形で管理しています。

現状は下記のようなTerraformのディレクトリ構成を採用しています。
このリポジトリ自体にはアプリケーションが一つしかないためrootにmodulesを配置し、環境固有のサブディレクトリにアプリケーションを分割しています。


```bash
.
└── terraform
    ├── environments
    │   ├── dev
    │   ├── prod
    │   └── stage
    └── modules
        └── gcp
            ├── xxx
            └── yyy
```
基本はGoogle Cloudが提唱しているベストプラクティスに乗っ取った形でリソースを作成しています。
https://cloud.google.com/docs/terraform/best-practices-for-terraform?hl=ja#subdirectories

この方法だと、単一Cloud Providerでの管理自体は簡単ですが、マルチクラウドの展開を見据えた際に、ライフサイクルの異なるプロバイダーごとのリソースを扱いたいとなった際のState管理が難しくなってしまうという問題点があるので、今後の課題として考えています。


k8sのマニフェストは、`Kustomize`を利用してリソースを管理しています。
構成シンプルにアプリケーションごと(API/DB Migration用のJob)等に分割しています。


```bash
.
└── manifests
    ├── argocd
    │   ├── base
    │   └── overlays
    │       ├── dev
    │       ├── prod
    │       └── stage
    └── app1
        ├── base
        └── overlays
            ├── dev
            ├── prod
            └── stage
```

特質すべき点は特にないですが、CRDとして、ExternalSecretOperatorを使用し、Secretの管理を行っています。
また、元々ArgoCDを利用していたため、ArgoCDの管理下にリソースを置く形でアプリケーションのリソースを運用しています。

![alt text](/images/aiworkerv2/2.png)


#### 3. アプリケーションレイヤーの修正

Google CloudでもAzure環境同様の動作を行うためにアプリケーションレイヤーの修正を行いました。
幸いなことに、interfaceで抽象化していたため、infra層の実体部分を修正だけで完了しました。

```ts:interface-example.ts
interface IStorageClient {
  retrieveBlob(params: RetrieveBlobParams): Promise<Result<Buffer>>
  uploadBlobsInBatch(params: UploadBlobBatchParams): Promise<Result<void>>
  deleteBlobsInBatch(params: DeleteBlobParams): Promise<Result<void>>
  generatePresignedUrls(
    params: GeneratePresignedUrlsParams
  ): Promise<Result<GetPresignedUrlsResponse>>
}
```

#### 4. CI/CDの修正

弊社ではGithub Actionsを使用してCI/CD Pipelineを構築しています。
元々Azure用にimageをビルドし、Azure Container Registryにpush、その後Rolloutを行っていましたが、Google Cloudに移行するにあたり、Artifact Registryにpushするように変更しています。

```yaml:sample.yaml
name: 'Deploy to GKE'
description: 'Composite action to deploy to GKE'
inputs:
  workload_identity_provider:
    description: 'Workload identity provider'
    required: true
  service_account:
    description: 'Service account'
    required: true
  artifact_repository:
    description: 'Artifact repository'
    required: true
  gcp_project:
    description: 'GCP project'
    required: true
  repository_name:
    description: 'Repository name'
    required: true
  gke_cluster:
    description: 'GKE cluster'
    required: true
  region:
    description: 'Region'
    required: true
  dockerfile:
    description: 'Path to the Dockerfile'
    required: true
  image_name:
    description: 'Image name'
    required: true
  image_tag:
    description: 'image tag'
    required: true
  target:
    description: 'Build target'
    required: true
  working_directory:
    description: 'Working directory'
    required: true

runs:
  using: 'composite'
  steps:
    - name: Get Bun version
      id: get_bun_version
      shell: bash
      run: echo "BUN_VERSION=$(grep 'bun = ' .prototools | cut -d '"' -f 2)" >> $GITHUB_ENV

    - name: Authorize Docker
      shell: bash
      run: gcloud auth configure-docker ${{ inputs.artifact_repository }} --quiet


    - name: Build and push Docker image to Artifact Registry
      shell: bash
      run: |
        IMAGE_TAG=$(git rev-parse --short "$GITHUB_SHA")
        docker build --target ${{ inputs.target }} --build-arg BUN_VERSION=${{ env.BUN_VERSION }} -t ${{ inputs.artifact_repository }}/${{ inputs.gcp_project }}/${{ inputs.repository_name }}/${{ inputs.image_name }}:${{ inputs.image_tag }} -f ${{ inputs.dockerfile }} .
        docker tag ${{ inputs.artifact_repository }}/${{ inputs.gcp_project }}/${{ inputs.repository_name }}/${{ inputs.image_name }}:${{ inputs.image_tag }} ${{ inputs.artifact_repository }}/${{ inputs.gcp_project }}/${{ inputs.repository_name }}/${{ inputs.image_name }}:latest
        docker push ${{ inputs.artifact_repository }}/${{ inputs.gcp_project }}/${{ inputs.repository_name }}/${{ inputs.image_name }}:${{ inputs.image_tag }}
      working-directory: ${{ inputs.working_directory }}
```

詳細は[#CI/CDの改善](#CI/CDの改善)に記載していますが、複数のビルドプロセスを並列で実行するために、Composite Actionに切り出し、運用を行っています。

### k8sリソースをArgoCDで管理

Google Cloudへの展開にあたりk8s上にリソースを構築しました。
我々が開発しているアプリケーション自体は、既存のApplication Clusterに乗せて、Namespaceを分ける形で運用しています。
また、ArgoCDではProjectを分けて、特定のnamespaceを操作できるように権限を管理し、リソースの論理的区分を行っています。

![alt text](/images/aiworkerv2/3.png)

また、ArgoCDのApplicationはApplicationSetを使用して管理しています。
https://argo-cd.readthedocs.io/en/stable/user-guide/application-set/

dev環境は頻繁にデプロイが起き、短時間のダウンタイムは許容できるため、AutoSyncを有効にすることで、リソースの同期を自動化しています。

```yaml:sample.yaml
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: hoge-namespace-dev-application-set
spec:
  generators:
    - list:
        elements:
          - path: app1

  template:
    metadata:
      name: '{{path}}-dev'
      namespace: default
    spec:
      source:
        path: deployment/k8s/manifests/{{path}}/overlays/dev
        repoURL: xxx
        targetRevision: development
      destination:
        namespace: hoge-namespace
        server: xxx
      project: 'hoge-namespace-dev'
      syncPolicy:
        automated:
          prune: true
          selfHeal: true
```
### デプロイの自動化について

元々アプリケーションはimageのlatest tagによる運用を行っていましたが、障害時の切り戻しや、リソースの状態の確認がしづらかったため、Tagを固定するように変更しました。

そのため現在は、Github Actions経由でTagを書き換えて、PRを自動で作成するようなワークフローを作成することで切り戻しがしやすいような構成に変更しました。
これによってRollbackをしやすく、かつArgoCD側で自動的にRolloutを行えるような構成に変更することで、柔軟なデプロイフローにも対応できるようになりました。

![alt text](/images/aiworkerv2/4.png)


:::message alert
ArgoCDにはImage Updaterという拡張ツールがあり、デプロイされたコンテナイメージを自動で更新することができます。特段の理由がない限り、CI/CDに余分な権限を持たせないで済むためこちらのツールを利用することをおすすめします。

https://argocd-image-updater.readthedocs.io/en/stable/
https://zenn.dev/aishift/articles/3fdc453bb36679

:::

### Organization Repositoryにおける自動マージ

弊チームはアプリケーションのリリースにGit Flowを採用しています。
その過程で例えば、`developmentブランチ`から`stagingブランチ`にマージを行いますが、その際は`①コンフリクトが起きないようにmergeコミットでコミット積んだ状態でマージし`、その後`②複数のワークフローをトリガーする`ようにする必要があります。

この①②を自動化するためにGithub Appsの導入を行いました。

そもそもですが、Githubにはデフォルトで自動認証が行うことができるGITHUB_TOKEN シークレットが用意されています。
https://docs.github.com/ja/actions/security-for-github-actions/security-guides/automatic-token-authentication#about-the-github_token-secret

このSecretを利用し、Github Actionsによる自動マージを行うことができるのですが、再帰的にワークフローがトリガーされることを防ぐために、`GITHUB_TOKEN`を利用したタスクの場合には新しいワークフローがトリガーされないようになっています。

https://docs.github.com/ja/actions/writing-workflows/choosing-when-your-workflow-runs/triggering-a-workflow#triggering-a-workflow-from-a-workflow

このような背景があったため、①の自動マージを行い、かつその後に②のワークフローをトリガーするためには`GITHUB_TOKEN`を使用せず、`Github Apps`を作成し、そのトークンを利用してマージをする必要がありました。
https://docs.github.com/ja/enterprise-cloud@latest/apps/creating-github-apps/authenticating-with-a-github-app/making-authenticated-api-requests-with-a-github-app-in-a-github-actions-workflow

:::message alert
PAT(Personal Access Token)を利用しても同様のことができますが、PATはユーザーの権限を持っているため、セキュリティ上のリスクがあるため、Github Appsを利用することをおすすめします。
:::

OrganizationにGithub Appsを導入したら、Github Appsから取得したトークンをstepで指定することで、Github Appsによるマージを行うことができます。

```yaml:sample.yaml
name: Auto Merge

on:
  pull_request:
    branches:
      - development

permissions:
  pull-requests: write

jobs:
  auto-merger-dev:
    runs-on: ubuntu-latest
    if: ${{ github.head_ref == 'staging' || github.head_ref == 'main' }}
    steps:
      - name: generate access token
        id: create
        uses: actions/create-github-app-token@v1
        with:
          app-id: ${{ secrets.TEST_APP_ID }}
          private-key: ${{ secrets.TEST_APP_PRIVATE_KEY }}
      - name: Checkout
        uses: actions/checkout@v4
        with:
          ref: staging
          token: ${{ steps.create.outputs.token }}
      - name: Auto-merge
        run: gh pr merge --auto --merge "$PR_URL"
        env:
          PR_URL: ${{github.event.pull_request.html_url}}
          GH_TOKEN: ${{ steps.create.outputs.token }}
```

以上のようにActionsによる自動マージから他のワークフローをトリガーするには少し手間がかかりますが、一度導入してしまえばその後は使いまわせるので非常に便利です。

:::message alert
Webで検索を行うとワークフローの構築方法は様々出てきますが、基本的にはサードパーティのActionsは使用せず、使用するとしてもVerified creatorのActionsを使用するのをおすすめします。今回のGithub Appsの例でもVerified creatorのActionsを使用しています。
https://github.com/marketplace/actions/create-github-app-token

また可能であれば、使用するActionsはFull Changeset Hashでバージョンを固定しておくと、セキュリティリスクを軽減できます。
:::

### 証明書の管理

弊チームではDNSをCloudflare、証明書をGoogle Managed Certificateで管理しています。
現状、CloudflareではDNS管理のみ行っているため、Proxyモードは利用していません。
本当はネットワークレイヤーをよりセキュアにできるのでProxyモードで運用したいところですが、Ingress周りの構成を大きく変更する必要があったので、一旦DNSのみの運用としています。
![alt text](/images/aiworkerv2/5.png)

:::message info
Proxyモードを利用することでDNSリゾルバに対する名前解決の結果CloudflareのIPアドレスが応答されるようになります。
https://zenn.dev/kameoncloud/articles/6dec28de015f6f
:::

## ロギングとモニタリングの改善と課題

正直なところモニタリングに関してはかなりの改善の余地があります。
現状各チームでのモニタリングの取り組みが異なるため、統一されたモニタリングの取り組みが必要だと感じています。

弊チームでは必要最低限Google Cloud Observabilityに基づきログの収集を行っています。

### 構造化ロギングとtsLog

アプリケーションではtslogを利用して構造化ロギングを行っています。

https://tslog.js.org/

pinoやwinston等とは異なり、コードベースがTypeScriptなため、何かあった際にコードが追いやすいのと、ログマスキング等が用意に行えるため使用しています。

さて、単純に構造化ロギングを行うだけであれば、loggerをimportし使用するだけですが、基本的には各Cloud Providerのログ収集サービスでは特定のフィールドに応じてUI上での視認性を向上させることが可能です。

例えば、Google Cloud Loggingでは特別なJSONフィールドが定義されています。
https://cloud.google.com/logging/docs/structured-logging?hl=ja

Cloud Providerに合わせて補正できるように、フィールドをフォーマットを変更するようにしています。

```ts:example.ts
interface CustomLogger {
  debug(logObj: InputLogObject): void
  info(logObj: InputLogObject): void
  warn(logObj: InputLogObject): void
  error(logObj: InputLogObject): void
}

interface InputLogObject {
  message: string
  labels?: { [key: string]: string }
  stackTrace?: string
  [key: string]: unknown
}

interface FormattedLogObject {
  message: string
  severity: string
  time: string // RFC3339
  requestId: string
  stack_trace?: string // Error stack trace
}

const createLoggerConfig = (env: LogEnv) =>
  ({
    name: 'app',
    type: env.LOG_TYPE,
    minLevel: env.LOG_MINLEVEL,
    hideLogPositionForProduction: env.LOG_HIDE_POSITION,
  }) satisfies ISettingsParam<FormattedLogObject>

class AppLogger implements CustomLogger {
  private loggerInstance: TSLogger<FormattedLogObject>
  private requestId: string

  constructor(opts: { env: LogEnv; requestId: string }) {
    const loggerConfig = createLoggerConfig(opts.env)
    this.loggerInstance = new TSLogger(loggerConfig)
    this.requestId = opts?.requestId || ''
  }

  private formatLogObject(inputLogObj: InputLogObject, severity: string): FormattedLogObject {
    return {
      ...inputLogObj,
      severity,
      time: getCurrentUTCDate().toISOString(),
      requestId: this.requestId,
      stack_trace: inputLogObj.stackTrace,
    }
  }

  debug(logObj: InputLogObject): void {
    const formattedLogObject = this.formatLogObject(logObj, 'DEBUG')
    this.loggerInstance.debug(formattedLogObject)
  }

  info(logObj: InputLogObject): void {
    const formattedLogObject = this.formatLogObject(logObj, 'INFO')
    this.loggerInstance.info(formattedLogObject)
  }

  warn(logObj: InputLogObject): void {
    const formattedLogObject = this.formatLogObject(logObj, 'WARNING')
    this.loggerInstance.warn(formattedLogObject)
  }

  error(logObj: InputLogObject): void {
    const formattedLogObject = this.formatLogObject(logObj, 'ERROR')
    this.loggerInstance.error(formattedLogObject)
  }
}
```


ここまでで、ログの視認性の向上は行えましやが、トレーサビリティを向上させるためには、特定のリクエストに対してのログをまとめる必要があります。その際にはrequestId等をLoggerのコンテキストに含めることが必要です。

弊チームではWebフレームワークにHonoを使用しており、middlewareを利用してloggerにrequestIdを付与し、Contextで引き回すようにしています。

https://zenn.dev/aishift/articles/a3dc8dcaac6bfa#%E6%A7%8B%E9%80%A0%E5%8C%96%E3%83%AD%E3%82%AE%E3%83%B3%E3%82%B0%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6

今までHonoはContextにアクセスするにはContextを各処理に伝搬していく必要がありましたが、このContext Storage Middlewareを利用することによって、Contextを各層で引き回さずともグローバルに扱えるようになっています。

https://hono.dev/docs/middleware/builtin/context-storage

:::message
Context Storage Middlewareは内部的にAsyncLocalStorageを利用しています。これはHonoのWeb標準に準拠に反しているのではないか?と思いますが、Node.js、Cloudflare Workers、Deno、Bunといった主要なランタイムで既に実装されており、実際には非標準APIでありながら多くの環境で利用可能なAPIであったり、WinterCG minimum specに提案されていたりと、今後の標準化が期待されるAPIなため、採用されているようです。

https://github.com/orgs/honojs/discussions/3409
:::

このContextにloggerを詰めておくことで、リクエストに対して一意なrequestIdを付与し、かつグローバルにLoggerが利用可能になります。

## リソースの最適化(Memory)
ある程度アプリケーションが出来上がってくると次はリソースの最適化が必要になりました。

主に問題になっていたのは下記です。
- **1.コンテナ起動時のメモリ使用量が高い**
- **2.特定のAPIを叩いた際にCPUとMemoryが急上昇する**

### 1.コンテナ起動時のメモリ使用量が高い

これはベースのDocker Imageが大きいことが原因でした。
ここは愚直にbuildとマルチステージビルドの改善を行いました。

```Dockerfile:sample
ARG BUN_VERSION
FROM oven/bun:${BUN_VERSION} as builder
WORKDIR /app
COPY . .
RUN bun install --production --frozen-lockfile
RUN bun build --entrypoints cmd/server/index.ts --target bun --outdir ./out/server

FROM oven/bun:${BUN_VERSION}-slim AS app
ENV TINI_VERSION v0.19.0
ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini
RUN chmod +x /tini
COPY --from=builder --chown=nonroot:nonroot /app/node_modules/tiktoken/tiktoken_bg.wasm /app/node_modules/tiktoken/tiktoken_bg.wasm
COPY --from=builder --chown=nonroot:nonroot /app/out/server /app
ENV NODE_ENV=production \
  TZ=UTC
EXPOSE 3000
ENTRYPOINT ["/tini", "--"]
CMD ["bun", "/app/index.js"]
```

また、Node.js同様に`PID1`問題があるため、`tini`を利用してサブプロセスを起動するようにしています。

https://github.com/nodejs/docker-node/blob/main/docs/BestPractices.md#handling-kernel-signals

ベースのイメージサイズが下がりましたが、そもそもNode.jsに比べるとBunはHttp Server起動でのメモリ使用量が高かったり、`Testcontainers`などのライブラリを動かせなかったりと不便なことが多いので、どこかのタイミングでランタイムは`Node.js`に切り替えたいなと思っています。(個人的に)


### 2.特定のAPIを叩いた際にCPUとMemoryが急上昇する
AI関連のサービスを開発していると特定のLLMモデルを用いた際のトークン使用量を計算したくなります。
その際に、利用されるライブラリに`TikToken`というものがあります。

https://github.com/dqbd/tiktoken

このライブラリを使用した際に生成されるTiktokenオブジェクトのサイズが非常に大きく、使用したモデルごと`gpt-4o`/`gpt-4o-mini`にオブジェクトを生成する必要があり、メモリを逼迫する原因となっていました。

この問題に関しては根本解決が難しいので、不必要なオブジェクトの生成を防ぐようにロジックを設計したり、全体のサービス設計を見直し、Tiktokenを使用する責務があるサービスに機能を委譲することを検討しています。


### その他の最適化

##　DevOps
### CI/CDの改善

### インフラ周りのlinterの導入
弊チームは1Repository内にフロントエンド/バックエンド/インフラのコードが混在しています。アプリケーション周りはBiomeによって統一的にFormatとLintを行っていますが、インフラ周りに関しては手動のチェックが多かったため、Terraform Linterの導入を行いました。


TerraformのチェックにはPR作成時点で下記のチェックを行っています。
- `terraform fmt -check -recursive`
- `terraform validate -no-color`
- `terraform plan`

将来的には`tflint`を導入し、よりベストプラクティに沿った厳密なチェックを行いたいと考えています。
https://developer.hashicorp.com/terraform/language/style


k8sのマニフェストに関しては`kustomize build`を実行し、構文的に誤りがないことのみをチェックしています。

将来的には`kubeconform`を導入し、スキーマのチェックを行えるような環境を構築したいと考えています。
https://github.com/yannh/kubeconform


### Renovateの導入
ライブラリの自動更新を行うためにRenovateを導入を行いました。

DependaBot等と比べて、フロントエンド/バックエンドごとにライブラリアップデートの周期が異なるのでその辺り細かく設定を行え、かつBunにも対応しているため導入を行いました。

https://docs.renovatebot.com/modules/manager/bun/

- yarn.lock更新されない問題

Bunはpackage install時に`bun.lockb`の他にヒューマンリーダブルな`yarn.lock`を生成させることができますが、Renovateでのライブラリ更新時に、このyarn.lockが更新されない問題がありました。

https://github.com/renovatebot/renovate/issues/20065#issuecomment-1712582023

そのため、Renovateが挙げてきたPRに対してyarn.lockを更新するようなCIを組むこと対応しています。


## セキュリティ
### アプリレイヤー

開発環境のWebアプリケーションにはCloud Accessを導入し、社内環境からのアクセスのみを許可するようにしています。
![alt text](/images/aiworkerv2/6.png)

Cloudflare Accessは様々なIdPとのIntegrationが可能かつ、50人までは無料で使うことができます。

https://developers.cloudflare.com/cloudflare-one/identity/idp-integration/

また、アプリケーション内の認証機能として、**社内環境**では`Clerk`をIdPとして利用しています。


基本は一般的な認証フローでほぼClerk側でよしなにやってくれるのでフローを意識することは少ないですが、フロントエンド-バックエンド間はCross Originリクエストのため手動でのJWT検証を行っています。
https://clerk.com/docs/backend-requests/handling/manual-jwt

![alt text](/images/aiworkerv2/7.png)

Clerkで利用される[Session Tokens](https://clerk.com/docs/backend-requests/resources/session-tokens)はJWTの形式でRS256で署名された非対称トークンとして提供されます。
そのためJWKを取得し、トークンの検証を行う必要があります。

### SAML Federation
ClerkにはEnterprise ConnectionとしてSAML Federationがサポートされています。(Proプラン+Pluginが必要)

https://clerk.com/docs/authentication/saml/overview


弊チームでは`SP-initiated`でSSOをしており、社内の認証基盤との連携ができるようになっています。
個人的に便利だなと思ったのはIdentity ProviderのMetadataの設定をファイルアップロードでよしなに行えるのは便利だなと思いました。Auth0などと比べるとかなり設定が簡単な印象です。

![alt text](/images/aiworkerv2/8.png)


また、SAML検証段階ではMock SAMLを利用して動作を確認しています。

https://mocksaml.com/

### Clerkの制約
Clerkは1テナントに収まる認証であれば機能が充実していますが、ビジネス上の利用においては少し機能が不十分な点があります。

- データの保管場所
Privacy Policy曰く、データは米国またはその他の国に転送され、保存される可能性があるとのことです。つまり、Clerk上に作成したユーザー情報を日本国内に保存したい、という要件に対応することが難しいです。

>All information processed by us may be transferred, processed, and stored anywhere in the world, including, but not limited to, the United States or other countries, which may have data protection laws that are different from the laws where you live. We endeavor to safeguard your information consistent with the requirements of applicable laws.

https://clerk.com/legal/privacy

- 監査ログの取得
Clerk側にログを保持している旨は記載されていますが、現状ユーザー側でログの確認はできません。(RoadMapには予定されていますがまだ実装自体は先になりそうです。)


Clerkのロードマップは下記から確認することが可能です。利用を検討している方はご参考ください。

https://feedback.clerk.com/roadmap


## その他取り組み
https://zenn.dev/aishift/articles/f82ec60b1762a0
https://zenn.dev/aishift/articles/1df36e912eb271


## まとめ
半年間くらいで行った機能開発の合間に行った改善系の取り組みをまとめました。公開が難しいものも多いため、一部のみの公開となりましたが、何かしらの参考になれば幸いです。
